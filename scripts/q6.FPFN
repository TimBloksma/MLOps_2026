import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np
import sys
import os

# Add project root to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def load_best_model():
    """
    Load your champion model from the saved checkpoint
    Modify this based on your actual model architecture and checkpoint location
    """
    # Example: Load a simple CNN model
    class SimpleCNN(nn.Module):
        def __init__(self):
            super(SimpleCNN, self).__init__()
            self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
            self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
            self.fc1 = nn.Linear(64 * 32 * 32, 128)  # Assuming 32x32 images after pooling
            self.fc2 = nn.Linear(128, 2)
            self.pool = nn.MaxPool2d(2)
            self.relu = nn.ReLU()
            self.dropout = nn.Dropout(0.5)
            
        def forward(self, x):
            x = self.pool(self.relu(self.conv1(x)))
            x = self.pool(self.relu(self.conv2(x)))
            x = x.view(x.size(0), -1)
            x = self.relu(self.fc1(x))
            x = self.dropout(x)
            x = self.fc2(x)
            return x
    
    # Initialize model
    model = SimpleCNN()
    
    # Load checkpoint - UPDATE THIS PATH TO YOUR ACTUAL CHECKPOINT
    checkpoint_path = os.path.join('experiments', 'best_model.pt')
    if os.path.exists(checkpoint_path):
        checkpoint = torch.load(checkpoint_path, map_location=device)
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
    else:
        print(f"Warning: Checkpoint not found at {checkpoint_path}")
        print("Using untrained model for demonstration")
    
    model = model.to(device)
    model.eval()
    return model

def get_test_loader():
    """
    Create test data loader
    Modify this based on your actual data loading setup
    """
    # Import your actual data loading code
    try:
        from src.data.make_dataset import get_test_loader as get_test_loader_actual
        return get_test_loader_actual()
    except ImportError:
        # Fallback for demonstration
        print("Using dummy test loader - replace with your actual data loader")
        
        # Create dummy dataset
        from torch.utils.data import DataLoader, TensorDataset
        
        # Create dummy data (batch_size=1 for error analysis)
        dummy_images = torch.randn(100, 3, 96, 96)  # PCAM images are 96x96
        dummy_labels = torch.randint(0, 2, (100,))
        dummy_dataset = TensorDataset(dummy_images, dummy_labels)
        
        return DataLoader(dummy_dataset, batch_size=1, shuffle=False)

def visualize_false_positives_negatives(model, test_loader, num_samples=5):
    """
    Visualize false positives and false negatives
    """
    samples = []
    
    model.eval()
    with torch.no_grad():
        for i, (x, y) in enumerate(test_loader):
            if len(samples) >= 100:  # Collect up to 100 samples
                break
            
            x = x.to(device)
            y = y.to(device)
            
            logits = model(x)
            
            if logits.ndim == 2 and logits.shape[1] == 2:
                pred_label = torch.argmax(logits, dim=1)
                pred_proba = torch.softmax(logits, dim=1)[:, 1]
            else:
                pred_label = (torch.sigmoid(logits) > 0.5).int()
                pred_proba = torch.sigmoid(logits)
            
            samples.append({
                "doc_id": i,
                "y_true": int(y.item()),
                "y_pred": int(pred_label.item()),
                "y_proba": float(pred_proba.item()),
                "image": x.cpu()
            })
    
    # Separate false positives and false negatives
    false_positives = [
        s for s in samples if s["y_true"] == 0 and s["y_pred"] == 1
    ]
    
    false_negatives = [
        s for s in samples if s["y_true"] == 1 and s["y_pred"] == 0
    ]
    
    print(f"Total samples collected: {len(samples)}")
    print(f"False positives: {len(false_positives)}")
    print(f"False negatives: {len(false_negatives)}")
    
    # Take first N of each for visualization
    fp_5 = false_positives[:num_samples]
    fn_5 = false_negatives[:num_samples]
    
    # Create visualization
    fig, axes = plt.subplots(2, num_samples, figsize=(3*num_samples, 6))
    
    if num_samples == 1:
        axes = axes.reshape(2, 1)
    
    # Plot false positives
    for j, s in enumerate(fp_5):
        img = s["image"].squeeze().permute(1, 2, 0)
        # Normalize for display
        img = (img - img.min()) / (img.max() - img.min())
        
        axes[0, j].imshow(img.numpy())
        axes[0, j].set_title(f"FP (P={s['y_proba']:.2f})")
        axes[0, j].axis("off")
    
    # Plot false negatives
    for j, s in enumerate(fn_5):
        img = s["image"].squeeze().permute(1, 2, 0)
        # Normalize for display
        img = (img - img.min()) / (img.max() - img.min())
        
        axes[1, j].imshow(img.numpy())
        axes[1, j].set_title(f"FN (P={s['y_proba']:.2f})")
        axes[1, j].axis("off")
    
    plt.suptitle(f"Error Analysis: {num_samples} False Positives (top) and False Negatives (bottom)")
    plt.tight_layout()
    
    # Save the figure
    output_dir = "q6_outputs"
    os.makedirs(output_dir, exist_ok=True)
    plt.savefig(os.path.join(output_dir, "error_analysis.png"), dpi=150, bbox_inches='tight')
    plt.show()
    
    return false_positives, false_negatives

def analyze_data_slice(samples, slice_name="bright", threshold=0.6):
    """
    Analyze model performance on a specific data slice
    """
    slice_samples = []
    other_samples = []
    
    for s in samples:
        # Calculate image brightness (mean pixel value)
        img = s["image"].squeeze()
        brightness = img.mean().item()
        
        # Slice definition: bright images
        if brightness > threshold:
            slice_samples.append(s)
        else:
            other_samples.append(s)
    
    # Calculate metrics for slice vs overall
    def calculate_metrics(sample_list):
        if not sample_list:
            return 0, 0, 0
        
        correct = sum(1 for s in sample_list if s["y_true"] == s["y_pred"])
        total = len(sample_list)
        accuracy = correct / total if total > 0 else 0
        
        # Get FPs and FNs in this slice
        fps = sum(1 for s in sample_list if s["y_true"] == 0 and s["y_pred"] == 1)
        fns = sum(1 for s in sample_list if s["y_true"] == 1 and s["y_pred"] == 0)
        
        return accuracy, fps, fns
    
    slice_acc, slice_fp, slice_fn = calculate_metrics(slice_samples)
    other_acc, other_fp, other_fn = calculate_metrics(other_samples)
    all_acc, all_fp, all_fn = calculate_metrics(samples)
    
    print("\n" + "="*50)
    print(f"DATA SLICE ANALYSIS: '{slice_name}' images (brightness > {threshold})")
    print("="*50)
    print(f"Slice size: {len(slice_samples)} images ({len(slice_samples)/len(samples)*100:.1f}%)")
    print(f"Slice accuracy: {slice_acc:.3f}")
    print(f"Slice false positives: {slice_fp}")
    print(f"Slice false negatives: {slice_fn}")
    print(f"Overall accuracy: {all_acc:.3f}")
    print(f"Overall false positives: {all_fp}")
    print(f"Overall false negatives: {all_fn}")
    print("="*50)
    
    return slice_samples

def main():
    """Main function for Q6 error analysis"""
    print("Starting Q6: Model Slicing and Error Analysis")
    print(f"Using device: {device}")
    
    # 1. Load the best model
    print("\n1. Loading champion model...")
    model = load_best_model()
    
    # 2. Get test data loader
    print("2. Loading test data...")
    test_loader = get_test_loader()
    
    # 3. Visualize errors
    print("3. Collecting and visualizing errors...")
    false_positives, false_negatives = visualize_false_positives_negatives(model, test_loader, num_samples=5)
    
    # 4. Combine all samples for slice analysis
    samples = []
    with torch.no_grad():
        for i, (x, y) in enumerate(test_loader):
            if i >= 200:  # Limit for faster analysis
                break
            x = x.to(device)
            y = y.to(device)
            logits = model(x)
            
            if logits.ndim == 2 and logits.shape[1] == 2:
                pred_label = torch.argmax(logits, dim=1)
            else:
                pred_label = (torch.sigmoid(logits) > 0.5).int()
            
            samples.append({
                "doc_id": i,
                "y_true": int(y.item()),
                "y_pred": int(pred_label.item()),
                "image": x.cpu()
            })
    
    # 5. Analyze data slice
    print("\n4. Analyzing data slices...")
    slice_samples = analyze_data_slice(samples, slice_name="bright", threshold=0.6)
    
    print("\nQ6 analysis complete!")
    print(f"Visualization saved to: q6_outputs/error_analysis.png")

if __name__ == "__main__":
    main()
